{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcf0e37",
   "metadata": {},
   "source": [
    "## S3 Data Transfer\n",
    "\n",
    "This notebook will demonstrate uploading and downloading from an S3 store using hub temporary S3 session credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13151285",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "To get S3 session credentials for your workspace bucket:\n",
    "1. Visit https://eodatahub.org.uk/workspaces/\n",
    "2. Ensure correct workspace is selected (icons on far left)\n",
    "3. Select \"Credentials\" from the side bar\n",
    "4. Select the S3 Token tab\n",
    "5. Click \"Request Temporary AWS S3 Credentials\"\n",
    "\n",
    "You will receive credentials in the form:\n",
    "```\n",
    "Access Key ID: ASIAT...\n",
    "Secret Access Key: yADQk...\n",
    "Session Token: IQoJb...\n",
    "Expiration: 2025-04-09T11:39:10Z\n",
    "```\n",
    "\n",
    "You will need to copy the Access Key ID, Secret Access Key and Session token into a local enviornment file (.env). The file should be of the form:\n",
    "```\n",
    "ACCESS_KEY_ID=ASIAT...\n",
    "SECRET_ACCESS_KEY=yADQk..\n",
    "SESSION_TOKEN=IQoJb...\n",
    "```\n",
    "\n",
    "You will use this file to load the session credentials into the notebook in the next cell. Update the .env filename as required.\n",
    "\n",
    "Note that these session credentials only last an hour, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies, if required\n",
    "%pip install boto3 dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c14f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = \".env\"  # update file name as required\n",
    "\n",
    "# Load session credentials\n",
    "load_dotenv(env_file)\n",
    "\n",
    "# Check all required environment variables are present\n",
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")\n",
    "SESSION_TOKEN = os.environ.get(\"SESSION_TOKEN\")\n",
    "\n",
    "missing_vars = [k for k, v in {\n",
    "    \"ACCESS_KEY_ID\": ACCESS_KEY_ID, \n",
    "    \"SECRET_ACCESS_KEY\": SECRET_ACCESS_KEY, \n",
    "    \"SESSION_TOKEN\": SESSION_TOKEN,\n",
    "}.items() if not v]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"The following environment variables are missing:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\" - {var}\")\n",
    "else:\n",
    "    print(\"All required environment variables are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Set up the Boto3 client and authenticate with the session credentials.\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=SECRET_ACCESS_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    ")\n",
    "s3 = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "# Upload to bucket from local directory\n",
    "\n",
    "# Configure bucket details\n",
    "workspace = \"my-workspace\"\n",
    "bucket_name = \"workspaces-eodhp\"  # This is the prod bucket, needs to be updated for environment, e.g. \"workspaces-eodhp-staging\"\n",
    "prefix = f\"{workspace}\"  # you can add subdirectories to the end of this if you wish to upload to a subdirectory of your bucket, e.g. `f\"{workspace}/path/to/subdir\"`\n",
    "local_dir = \"local/path/to/dir\"  # local directory to be uploaded, update as required\n",
    "\n",
    "for root, dirs, files in os.walk(local_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_dir)\n",
    "        s3_path = os.path.join(prefix, relative_path).replace(\"\\\\\", \"/\")  # replace windows paths with posix\n",
    "\n",
    "        print(f\"Uploading {local_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        try:\n",
    "            s3.upload_file(local_path, bucket_name, s3_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {local_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6917f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "# Download an S3 prefix to local directory\n",
    "\n",
    "# Configure bucket details\n",
    "workspace = \"my-workspace\"\n",
    "bucket_name = \"workspaces-eodhp\"  # This is the prod bucket, needs to be updated for environment, e.g. \"workspaces-eodhp-staging\"\n",
    "prefix = f\"{workspace}\"  # you can add subdirectories to the end of this if you wish to download from a subdirectory of your bucket, e.g. `f\"{workspace}/path/to/subdir\"`\n",
    "local_dir = \"local/path/to/dir\"  # local directory to be downloaded to, update as required\n",
    "\n",
    "# Ensure local directory exists\n",
    "Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        s3_key = obj[\"Key\"]  # Full S3 path\n",
    "        relative_path = os.path.relpath(s3_key, prefix)  # Path relative to the prefix\n",
    "        local_path = os.path.join(local_dir, relative_path)  # Local file path\n",
    "\n",
    "        # Ensure local subdirectories exist\n",
    "        Path(os.path.dirname(local_path)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        print(f\"Downloading {s3_key} to {local_path}\")\n",
    "        s3.download_file(bucket_name, s3_key, local_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
